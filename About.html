<!DOCTYPE HTML>
<html>
	<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>SHIRLEY NIKKITHA - PYTHON PROJECTS</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="" />
	<meta name="keywords" content="" />
	<meta name="author" content="" />


	<link href="https://fonts.googleapis.com/css?family=Poppins:300,400,500,600,700" rel="stylesheet">
	
	<!-- Animate.css -->
	<link rel="stylesheet" href="css/animate.css">
	<!-- Icomoon Icon Fonts-->
	<link rel="stylesheet" href="css/icomoon.css">
	<!-- Bootstrap  -->
	<link rel="stylesheet" href="css/bootstrap.css">
	<!-- Magnific Popup -->
	<link rel="stylesheet" href="css/magnific-popup.css">
	<!-- Owl Carousel  -->
	<link rel="stylesheet" href="css/owl.carousel.min.css">
	<link rel="stylesheet" href="css/owl.theme.default.min.css">
	<!-- Flexslider  -->
	<link rel="stylesheet" href="css/flexslider.css">
	<!-- Flaticons  -->
	<link rel="stylesheet" href="fonts/flaticon/font/flaticon.css">
	<!-- Theme style  -->
	<link rel="stylesheet" href="css/style.css">

	<!-- Modernizr JS -->
	<script src="js/modernizr-2.6.2.min.js"></script>
	

	</head>
	<body>
            
        <div class="container">
               <div style='position:absolute;zindex:0;left:0;top:0;width:100%;height:200%'>
                    <img src='images/blue.gif' style='width:100%;height:170%' alt='[]' />
                </div>
            </div>
	<div class="colorlib-loader"></div>
	
	<div class="colorlib-loader"></div>
	
	<div id="page">
            
	<nav class="colorlib-nav" role="navigation">
		<div class="top-menu">
			<div class="container">
				<div class="row">
					<div class="col-xs-12">
						<div class="top">
							<div class="row">

							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="menu-wrap">
				<div class="container">
					<div class="row">
						<div class="col-xs-8">
							<div class="menu-1">
								<ul>
									<li class="active"><a href="index.html">Home</a></li>
									<li class="has-dropdown">
										<a href="UserLogin.jsp">Login</a>										
									</li>
									<li><a href="UserSignup.jsp">Register</a></li>
                                                                        <li><a href="Services.html">FAQ</a></li>
                                                                        <li><a href="Contact.html">Contact</a></li>
                                                                        <li><a href="About.html">About Us</a></li>
                                                                        <li><a href="Meet.html">Meet Us</a></li>
                                                                        
								</ul>
							</div>
                                                    
						</div>
					</div>
				</div>
			</div>
		</div>
        </nav><br>
        <h3> </h3>
        
  <div class="w3-container" id="contact" style="margin-top:75px">
    
    <br>
    <h1>My Projects List</h1>
       <ul>
<li><h2><a class="reference internal" href="#what-is-python" id="id4">PUPIL DETECTION ALGORITHM BASED ON FEATURE EXTRACTION FOR EYE GAZE</a></h2></li>
<li><h2><a class="reference internal" href="#what-is-py" id="id4">HAND GESTURE RECOGNITION</a></h2></li>
</ul>
  </div>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <br>
        <div class="section" id="general-information">
<h2><a class="toc-backref" href="#id3">About the Projects</a><a class="headerlink" href="#general-information" title="Permalink to this headline"></a></h2>
<div class="section" id="what-is-python">
    <br>
        <br>
        <br>
        <br>
        <br>
<h3><a class="toc-backref" href="#id4"></a>I.	PUPIL DETECTION ALGORITHM BASED ON FEATURE EXTRACTION FOR EYE GAZE<a class="headerlink" href="#what-is-python" title="Permalink to this headline"></a></h3>
<p>Exact real-time pupil tracking is an important step in a live eye gaze. Since pupil centre is a base point’s reference, exact eye centre localization is essential for many applications. <br>
    In this project, we extract pupil eye features exactly within different intensity levels of eye images, mostly with localization of determined interest objects and where the human is looking, since it’s a digital world and digital transformation, everything is becoming virtual. <br>
    Hence this concept has a huge scope in e-learning, class room training, analyse human behaviour.<br>
    This project covers main process like Eye Ball and mentality & mood Recognition of Human Beings. <br>
    Feature extraction method named Kalman ﬁlter is used for gaze movement process.<br>
    Harr’s cascade classifier was used to first locate the eye’s area, and once found and support vector machine (SVM) for classification with the trained datasets.<br>
    We also include the state of emotions facial landmarks of the salient patches on face image using automated learning-free facial landmark detection technique.<br>
    
In a virtual learning environment, learners can lose motivation and concentration easily, especially in a platform that is not tailored to their needs. <br>
Our research is based on studying learner’s behavior on an online learning platform to create a system able to clustering learners based on their behavior, and adapting educational content to their needs. <br>

Eye tracking is a set of techniques for recording eye movements.<br>
This technology is used to measure eye positions and eye movement for research in psychology, psycholinguistics, ergonomics, e-learning and pre-testing of advertising.<br>
This paper introduces the use of eye tracking technology to track and analyze the learners' behavior and emotion on e-learning platform like level of attention, stress, relaxation, problem solving and tiredness.<br>

In e-learning, it is necessary to create more effective interaction between the educational content and learners. <br>
In particular, increasing motivation by stimulating learners' interest is very important.<br>
Users' eyes can be a significant source of information to analyze learner behavior. <br>
What we look at, and how we do that, can be exploited to improve the learning process.<br>
Eye movements provide an indication of learner interest and focus of attention. <br>
They provide useful feedback to personalize learning interactions, and this can bring back some of the human functionality of a teacher. <br>
With a study of eye movement, learners may be more motivated, and may find learning more fun. <br>

From identification of where on the screen a user is looking, substantial value can be obtained.<br>
The main aim of our project is developing a technique of eye detection and gaze estimation which is evolved from interest in a novel interface for laptop computers. <br>
This interface would offers a user to control the computer through observing the features of interest and expresses action by blinking eyes. <br>
This technology is beneficial for detection of fatigueness, surveillance, human computer interaction, advertisement etc. <br>

For example: a driver are driving a car he could safely come to a stop should the driver experience a micro sleep. <br>
Then the buzzer is ON and give instruction like a drive is fatigueness. <br>
In human computer interface for a laptop the grinding technique are used and the indentification of pointer movement and we want to operate the laptop features in eye movement.<br>
In this project the technique was used and carried out the gaze estimation of an eye.<br>

Basically, eye detection technique is divided into three methods: a) Shape-based, b) Appearance-based; c) Hybrid shape and appearance-based. <br>
In shape-based method, the eye shape such as eye edges, pupil, and eye corners are used as the features.<br>
In appearance-based method, the photometric appearance of eye is used for detection. <br>
Hybrid method combines both shape and appearance methods to exploit their advantages.<br>
</p>
</div>
<div class="section" id="what-is-py">
<h3><a class="toc-backref" href="#id4"></a>II.	HAND GESTURE RECOGNITION<a class="headerlink" href="#what-is-python" title="Permalink to this headline"></a></h3>
<p>The goal for the project was to develop a new type of Human Computer Interaction system that subdues the problems that users have been facing with the current system. The project is implemented on a Li<br>
    nux system but could be implemented on a windows system by downloading some modules for python. <br>
    The algorithm applied is resistant to change in background image as it is not based on background image subtraction and is not programmed for a specific hand type; the algorithm used can process different hand types, recognizes no of fingers, and can carry out tasks as per requirement. <br>
    As it is stated within this paper, the main goals were reached.<br>
    The application is capable of the gesture recognition in real-time.<br>
    There are some limitations, which we still have to be overcome in future.<br>
    
In today‘s world, the computers have become an important aspect of life and are used in various fields however, the systems and methods that we use to interact with computers are outdated and have various issues, which we will discuss a little later in this paper. <br>
Hence, a very new field trying to overcome these issues has emerged namely HUMAN COMPUTER INTERACTIONS (HCI). <br>
Although, computers have made numerous advancement in both fields of Software and Hardware, Still the basic way in which Humans interact with computers remains the same, using basic pointing device (mouse) and Keyboard or advanced Voice Recognition System, or maybe Natural Language processing in really advanced cases to make this communication more human and easy for us.<br>

Our proposed project is the Hand gestures recognition system to replace the basic pointing devices used in computer systems to reduce the limitations that stay due to the legacy systems such as mouse and Touchpad. <br>
The proposed system uses hand gesture, mostly no of fingers raised within the region of Interest to perform various operations such as Mouse move, Left click and Right click. <br>
A static control board restrains the versatility of client and limits the capacity of the client like a remote can be lost, dropped or broken while, the physical nearness of client is required at sight of activity and that is a limitation of the user.<br>

The proposed system can be used to control various soft panels like HMI systems, Robotics Systems, Telecommunication System, using hand gestures with help of programming by within python using pyautogui module to facilitate interaction within different functions of computer through the Camera to capture video frames.<br>

</p>
</div>
        </div>
        
	
	
	<!-- jQuery -->
	<script src="js/jquery.min.js"></script>
	<!-- jQuery Easing -->
	<script src="js/jquery.easing.1.3.js"></script>
	<!-- Bootstrap -->
	<script src="js/bootstrap.min.js"></script>
	<!-- Waypoints -->
	<script src="js/jquery.waypoints.min.js"></script>
	<!-- Stellar Parallax -->
	<script src="js/jquery.stellar.min.js"></script>
	<!-- Carousel -->
	<script src="js/owl.carousel.min.js"></script>
	<!-- Flexslider -->
	<script src="js/jquery.flexslider-min.js"></script>
	<!-- countTo -->
	<script src="js/jquery.countTo.js"></script>
	<!-- Magnific Popup -->
	<script src="js/jquery.magnific-popup.min.js"></script>
	<script src="js/magnific-popup-options.js"></script>
	<!-- Sticky Kit -->
	<script src="js/sticky-kit.min.js"></script>
	<!-- Main -->
	<script src="js/main.js"></script>     
        <br>
        </body>
</html>